{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import digits\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize tokenizers\n",
    "word_tokenizer = Tokenizer(BPE(unk_token=\"?\"))\n",
    "phoneme_tokenizer = Tokenizer(BPE(unk_token=\"?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting out for now\n",
    "# from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'bout B AW1 T\",\n",
       " \"'cause K AH0 Z\",\n",
       " \"'course K AO1 R S\",\n",
       " \"'cuse K Y UW1 Z\",\n",
       " \"'em AH0 M\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "english = '../data/cmudict/cmudict.dict'\n",
    "data = []\n",
    "with open(english, 'r') as infile:\n",
    "        for line in infile:\n",
    "                    data.append((line.rstrip('\\n')))\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-english and extra words\n",
    "for item in data:\n",
    "    if '#' in item or '(' in item:\n",
    "        data.remove(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'bout B AW T\",\n",
       " \"'cause K AH Z\",\n",
       " \"'course K AO R S\",\n",
       " \"'cuse K Y UW Z\",\n",
       " \"'em AH M\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove numbers from phonemes\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i].translate(remove_digits)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add normalizers to remove accents from non-english words\n",
    "# from tokenizers.normalizers import NFD, StripAccents\n",
    "\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "word_tokenizer.pre_tokenizer = Whitespace()\n",
    "phoneme_tokenizer.pre_tokenizer = Whitespace()\n",
    "word_tokenizer.normalizer = normalizer\n",
    "phoneme_tokenizer.normalizer = normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable padding\n",
    "word_tokenizer.enable_padding(direction='right', pad_id=0, pad_type_id=0, \n",
    "               pad_token='*', length=None, pad_to_multiple_of=None)\n",
    "phoneme_tokenizer.enable_padding(direction='right', pad_id=0, pad_type_id=0, \n",
    "               pad_token='*', length=None, pad_to_multiple_of=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'bout\", \"'cause\", \"'course\", \"'cuse\", \"'em\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words only\n",
    "words = [item.split(' ')[0] for item in data]\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.append(['?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate by character\n",
    "words = [[letter for letter in item] for item in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['B', 'AW', 'T'],\n",
       " ['K', 'AH', 'Z'],\n",
       " ['K', 'AO', 'R', 'S'],\n",
       " ['K', 'Y', 'UW', 'Z'],\n",
       " ['AH', 'M']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get phonemes\n",
    "phonemes = [item.split(' ')[1:] for item in data]\n",
    "phonemes.append(['start', 'stop'])\n",
    "phonemes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Tokenizers\n",
    "word_tokenizer.train_from_iterator(words)\n",
    "phoneme_tokenizer.train_from_iterator(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Tokenizers\n",
    "word_test = word_tokenizer.encode(''.join(words[203]))\n",
    "phoneme_test = phoneme_tokenizer.encode(words[48][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'i', 'm', 'a', 'e', 'l', 's']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_test.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 14, 18, 6, 10, 17, 24]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_test.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneme_test.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneme_test.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizers\n",
    "word_tokenizer.save(\"../data/token_encodings/word_tokenizer-eng.json\")\n",
    "phoneme_tokenizer.save('../data/token_encodings/phoneme_tokenizer-eng.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>phonemes</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abadžija</td>\n",
       "      <td>NaN</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abadžija</td>\n",
       "      <td>start</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abadžija</td>\n",
       "      <td>start B</td>\n",
       "      <td>AE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abadžija</td>\n",
       "      <td>start B AE</td>\n",
       "      <td>JH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abadžija</td>\n",
       "      <td>start B AE JH</td>\n",
       "      <td>IH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word       phonemes  label\n",
       "0  abadžija            NaN  start\n",
       "1  abadžija          start      B\n",
       "2  abadžija        start B     AE\n",
       "3  abadžija     start B AE     JH\n",
       "4  abadžija  start B AE JH     IH"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on different language\n",
    "cro_df = pd.read_csv('../data/model_ready/csv/processed_croatian.csv', index_col = 0)\n",
    "cro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cro_test = word_tokenizer.encode(list(cro_df['word'].values)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'a', 'd', 'z', 'i', 'j', 'a']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cro_test.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 6, 9, 31, 14, 15, 6]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cro_test.ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
