{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import digits\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize tokenizers\n",
    "# word_tokenizer = Tokenizer(BPE(unk_token=\"?\"))\n",
    "# phoneme_tokenizer = Tokenizer(BPE(unk_token=\"?\"))\n",
    "word_tokenizer = Tokenizer(BPE())\n",
    "phoneme_tokenizer = Tokenizer(BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting out for now\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"?\", \"*\"], unk_token='?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note by Antonio `10/5/21` : edited the previous code because it wasn't excluding the words as intended (no idea why the previous code didn't work as expected but below accomplishes what we want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "english = '../data/cmudict/cmudict.dict'\n",
    "data = []\n",
    "\n",
    "with open(english) as f:\n",
    "        lines = f.readlines()\n",
    "for line in lines:\n",
    "    pairs = line.strip('\\n').split(' ', 1)\n",
    "    if re.search(r'(\\d)', pairs[0]) or '#' in pairs[1]:\n",
    "        # skipping any alternate pronunciations, which are denoted by (2) or (3)\n",
    "        # similarly, skipping any words of foreign descent, denoted by '#'\n",
    "        continue\n",
    "    else:\n",
    "        # remove \n",
    "        pairs[1] = re.sub(r'\\d', '', pairs[1])\n",
    "        pairs[0] = re.sub(r'[^A-Za-z\\s]', '?', pairs[0])\n",
    "        data.append(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['?bout', 'B AW T'],\n",
       " ['?cause', 'K AH Z'],\n",
       " ['?course', 'K AO R S'],\n",
       " ['?cuse', 'K Y UW Z'],\n",
       " ['?em', 'AH M']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add normalizers to remove accents from non-english words\n",
    "# from tokenizers.normalizers import NFD, StripAccents\n",
    "\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "word_tokenizer.pre_tokenizer = Whitespace()\n",
    "phoneme_tokenizer.pre_tokenizer = Whitespace()\n",
    "word_tokenizer.normalizer = normalizer\n",
    "phoneme_tokenizer.normalizer = normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable padding\n",
    "word_tokenizer.enable_padding(direction='right', pad_id=0, pad_type_id=0, \n",
    "               pad_token='*', length=None, pad_to_multiple_of=None)\n",
    "phoneme_tokenizer.enable_padding(direction='right', pad_id=0, pad_type_id=0, \n",
    "               pad_token='*', length=None, pad_to_multiple_of=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['?bout', '?cause', '?course', '?cuse', '?em']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words only\n",
    "words = [item[0] for item in data]\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words.append(['?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate by character\n",
    "words = [[letter for letter in item] for item in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B AW T', 'K AH Z', 'K AO R S', 'K Y UW Z', 'AH M']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get phonemes\n",
    "phonemes = [item[1] for item in data]\n",
    "phonemes.append(['start', 'stop'])\n",
    "phonemes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Tokenizers\n",
    "word_tokenizer.train_from_iterator(words, trainer=trainer)\n",
    "phoneme_tokenizer.train_from_iterator(phonemes, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Tokenizers\n",
    "word_test = word_tokenizer.encode(''.join(words[203]))\n",
    "phoneme_test = phoneme_tokenizer.encode(words[48][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'i', 'o']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_test.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 10, 16]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_test.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneme_test.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneme_test.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizers\n",
    "word_tokenizer.save(\"../data/token_encodings/word_tokenizer-eng.json\")\n",
    "phoneme_tokenizer.save('../data/token_encodings/phoneme_tokenizer-eng.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>phonemes</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abadžija</td>\n",
       "      <td>start</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abadžija</td>\n",
       "      <td>start B</td>\n",
       "      <td>AE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abadžija</td>\n",
       "      <td>start B AE</td>\n",
       "      <td>JH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abadžija</td>\n",
       "      <td>start B AE JH</td>\n",
       "      <td>IH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abadžija</td>\n",
       "      <td>start B AE JH IH</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word          phonemes label\n",
       "0  abadžija             start     B\n",
       "1  abadžija           start B    AE\n",
       "2  abadžija        start B AE    JH\n",
       "3  abadžija     start B AE JH    IH\n",
       "4  abadžija  start B AE JH IH     Y"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on different language\n",
    "cro_df = pd.read_csv('../data/model_ready/csv/processed_croatian.csv')\n",
    "cro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cro_test = word_tokenizer.encode_batch(list(cro_df['word'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 2, 5, 27, 10, 11, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cro_test[0].ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'a',\n",
       " 'd',\n",
       " 'z',\n",
       " 'i',\n",
       " 'j',\n",
       " 'a',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cro_test[0].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
